{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aac9b2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data..\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/train.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9340\\1087604008.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    340\u001b[0m     \u001b[0marg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mArgs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m     \u001b[0mrandom_seed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRANDOM_STATE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m     \u001b[0mtest_preds\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Downloads\\Compressed\\Handwritten-Digit-Recognition-CNN-Flask-App-\\Handwritten-Digit-Recognition-CNN-Flask-App-\\utils.py\u001b[0m in \u001b[0;36mwrapper_timer\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwrapper_timer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[0mstart_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m         \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperf_counter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m         \u001b[0mhours\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdivmod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3600\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9340\\1087604008.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(args)\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[1;31m# reading train and test data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    261\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Reading Data..'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 262\u001b[1;33m     \u001b[0mdfx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'train.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    263\u001b[0m     \u001b[0mdf_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata_path\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'test.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m                 )\n\u001b[1;32m--> 311\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    312\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    313\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    676\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 678\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    679\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    680\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 575\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[1;33m|\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 932\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    933\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    934\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m             \u001b[1;31m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1215\u001b[0m             \u001b[1;31m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1216\u001b[1;33m             self.handles = get_handle(  # type: ignore[call-overload]\n\u001b[0m\u001b[0;32m   1217\u001b[0m                 \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1218\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;34m\"b\"\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    785\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 786\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    787\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/train.csv'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils import data\n",
    "\n",
    "from utils import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import makedirs\n",
    "from typing import Union\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "class MnistModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Custom CNN Model for Mnist\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, classes: int) -> None:\n",
    "        super(MnistModel, self).__init__()\n",
    "\n",
    "        self.classes = classes\n",
    "\n",
    "        # initialize the layers in the first (CONV => RELU) * 2 => POOL + DROP\n",
    "        # (N,1,28,28) -> (N,16,24,24)\n",
    "        self.conv1A = nn.Conv2d(\n",
    "            in_channels=1, out_channels=16, kernel_size=5, stride=1, padding=0)\n",
    "        # (N,16,24,24) -> (N,32,20,20)\n",
    "        self.conv1B = nn.Conv2d(\n",
    "            in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=0)\n",
    "        # (N,32,20,20) -> (N,32,10,10)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        self.act = nn.ReLU()\n",
    "        self.do = nn.Dropout(0.25)\n",
    "\n",
    "        # initialize the layers in the second (CONV => RELU) * 2 => POOL + DROP\n",
    "        # (N,32,10,10) -> (N,64,8,8)\n",
    "        self.conv2A = nn.Conv2d(\n",
    "            in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=0)\n",
    "        # (N,64,8,8) -> (N,128,6,6)\n",
    "        self.conv2B = nn.Conv2d(\n",
    "            in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=0)\n",
    "        # (N,128,6,6) -> (N,128,3,3)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=2)\n",
    "\n",
    "        # initialize the layers in our fully-connected layer set\n",
    "        # (N,128,3,3) -> (N,32)\n",
    "        self.dense3 = nn.Linear(128*3*3, 32)\n",
    "\n",
    "        # initialize the layers in the softmax classifier layer set\n",
    "        # (N, classes)\n",
    "        self.dense4 = nn.Linear(32, self.classes)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "\n",
    "        # build the first (CONV => RELU) * 2 => POOL layer set\n",
    "        x = self.conv1A(x)\n",
    "        x = self.act(x)\n",
    "        x = self.conv1B(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pool1(x)\n",
    "        x = self.do(x)\n",
    "\n",
    "        # build the second (CONV => RELU) * 2 => POOL layer set\n",
    "        x = self.conv2A(x)\n",
    "        x = self.act(x)\n",
    "        x = self.conv2B(x)\n",
    "        x = self.act(x)\n",
    "        x = self.pool2(x)\n",
    "        x = self.do(x)\n",
    "\n",
    "        # build our FC layer set\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dense3(x)\n",
    "        x = self.act(x)\n",
    "        x = self.do(x)\n",
    "\n",
    "        # build the softmax classifier\n",
    "        x = nn.functional.log_softmax(self.dense4(x), dim=1)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class MnistDataset(data.Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for Mnist\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, df: pd.DataFrame, target: np.array, test: bool = False) -> None:\n",
    "        self.df = df\n",
    "        self.test = test\n",
    "\n",
    "        # if test=True skip this step\n",
    "        if not self.test:\n",
    "            self.df_targets = target\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        # return length of the dataset\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Union[tuple, torch.Tensor]:\n",
    "        # if indexes are in tensor, convert to list\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # if test=False return bunch of images, targets\n",
    "        if not self.test:\n",
    "            return torch.as_tensor(self.df[idx].astype(float)), self.df_targets[idx]\n",
    "        # if test=True return only images\n",
    "        else:\n",
    "            return torch.as_tensor(self.df[idx].astype(float))\n",
    "\n",
    "\n",
    "def loss_fn(outputs: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Loss Function\n",
    "\n",
    "    Args:\n",
    "        outputs (torch.Tensor): Predicted Labels\n",
    "        targets (torch.Tensor): True Labels\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: NLLLoss value\n",
    "    \"\"\"\n",
    "    return nn.NLLLoss()(outputs, targets)\n",
    "\n",
    "\n",
    "def train_loop_fn(data_loader, model, optimizer, device, scheduler=None):\n",
    "    \"\"\"\n",
    "    Training Loop\n",
    "\n",
    "    Args:\n",
    "        data_loader: Train Data Loader\n",
    "        model: NN Model\n",
    "        optimizer: Optimizer\n",
    "        device: Device (CPU/CUDA)\n",
    "        scheduler: Scheduler. Defaults to None.\n",
    "    \"\"\"\n",
    "    # set model to train\n",
    "    model.train()\n",
    "    # iterate over data loader\n",
    "    train_loss = []\n",
    "    for ids, targets in data_loader:\n",
    "        # sending to device (cpu/gpu)\n",
    "        ids = ids.to(device, dtype=torch.float)\n",
    "        targets = targets.to(device, dtype=torch.long)\n",
    "\n",
    "        # Clear gradients w.r.t. parameters\n",
    "        optimizer.zero_grad()\n",
    "        # Forward pass to get output/logits\n",
    "        outputs = model(x=ids)\n",
    "        # Calculate Loss: softmax --> negative log likelihood loss\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        train_loss.append(loss)\n",
    "        # Getting gradients w.r.t. parameters\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if scheduler is not None:\n",
    "            # Updating scheduler\n",
    "            if type(scheduler).__name__ == 'ReduceLROnPlateau':\n",
    "                scheduler.step(loss)\n",
    "            else:\n",
    "                scheduler.step()\n",
    "    print(f\"Loss on Train Data : {sum(train_loss)/len(train_loss)}\")\n",
    "\n",
    "\n",
    "def eval_loop_fn(data_loader, model, device):\n",
    "    \"\"\"\n",
    "    Evaluation Loop\n",
    "\n",
    "    Args:\n",
    "        data_loader: Evaluation Data Loader\n",
    "        model: NN Model\n",
    "        device: Device (CPU/CUDA)\n",
    "\n",
    "    Returns:\n",
    "        List of Target Labels and True Labels\n",
    "    \"\"\"\n",
    "    # full list of targets, outputs\n",
    "    fin_targets = []\n",
    "    fin_outputs = []\n",
    "    # set model to eveluate\n",
    "    model.eval()  # as model is set to eval, there will be no optimizer and scheduler update\n",
    "\n",
    "    # iterate over data loader\n",
    "    for _, (ids, targets) in enumerate(data_loader):\n",
    "        ids = ids.to(device, dtype=torch.float)\n",
    "        targets = targets.to(device, dtype=torch.long)\n",
    "\n",
    "        outputs = model(x=ids)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "        loss.backward()\n",
    "\n",
    "        # Get predictions from the maximum value\n",
    "        _, outputs = torch.max(outputs.data, 1)\n",
    "\n",
    "        # appending the values to final lists\n",
    "        fin_targets.append(targets.cpu().detach().numpy())\n",
    "        fin_outputs.append(outputs.cpu().detach().numpy())\n",
    "    return np.vstack(fin_outputs), np.vstack(fin_targets)\n",
    "\n",
    "\n",
    "def test_loop_fn(test, model, device):\n",
    "    \"\"\"\n",
    "    Testing Loop\n",
    "\n",
    "    Args:\n",
    "        test: Test DataFrame\n",
    "        model: NN Model\n",
    "        device: Device (CPU/CUDA)\n",
    "\n",
    "    Returns:\n",
    "        List of Predicted Labels\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    # convert test data to FloatTensor\n",
    "    test = torch.as_tensor(test)\n",
    "    test = test.to(device, dtype=torch.float)\n",
    "\n",
    "    # Get predictions\n",
    "    pred = model(test)\n",
    "    # Get predictions from the maximum value\n",
    "    _, predlabel = torch.max(pred.data, 1)\n",
    "    # converting to list\n",
    "    predlabel = predlabel.tolist()\n",
    "\n",
    "    # Plotting the predicted results\n",
    "    L = 5\n",
    "    W = 5\n",
    "    _, axes = plt.subplots(L, W, figsize=(12, 12))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    for i in np.arange(0, L * W):\n",
    "        axes[i].imshow(test[i].cpu().detach().numpy().reshape(28, 28))\n",
    "        axes[i].set_title(\"Prediction Class = {:0.1f}\".format(predlabel[i]))\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.suptitle('Predictions on Test Data')\n",
    "    plt.subplots_adjust(wspace=0.5)\n",
    "    plt.show()\n",
    "\n",
    "    return predlabel\n",
    "\n",
    "\n",
    "@timer\n",
    "def run(args):\n",
    "    \"\"\"\n",
    "    Function where all the magic happens\n",
    "\n",
    "    Args:\n",
    "        args: Arguments for Training\n",
    "\n",
    "    Returns:\n",
    "        List of Predicted Labels\n",
    "    \"\"\"\n",
    "    # reading train and test data\n",
    "    print('Reading Data..')\n",
    "    dfx = pd.read_csv(args.data_path+'train.csv')\n",
    "    df_test = pd.read_csv(args.data_path+'test.csv')\n",
    "\n",
    "    classes = dfx[args.target].nunique()\n",
    "\n",
    "    print('Data Wrangling..')\n",
    "    # spliting train data to train, validate\n",
    "    split_idx = int(len(dfx) * 0.8)\n",
    "    df_train = dfx[:split_idx].reset_index(drop=True)\n",
    "    df_valid = dfx[split_idx:].reset_index(drop=True)\n",
    "\n",
    "    # target labels\n",
    "    train_targets = df_train[args.target].values\n",
    "    valid_targets = df_valid[args.target].values\n",
    "\n",
    "    # reshaping data to 28 x 28 images and normalize\n",
    "    df_train = df_train.drop(args.target, axis=1).values.reshape(\n",
    "        len(df_train), 1, 28, 28)/255\n",
    "    df_valid = df_valid.drop(args.target, axis=1).values.reshape(\n",
    "        len(df_valid), 1, 28, 28)/255\n",
    "    df_test = df_test.values.reshape(len(df_test), 1, 28, 28)/255\n",
    "\n",
    "    print('DataSet and DataLoader..')\n",
    "    # Creating PyTorch Custom Datasets\n",
    "    train_dataset = MnistDataset(df=df_train, target=train_targets)\n",
    "    valid_dataset = MnistDataset(df=df_valid, target=valid_targets)\n",
    "\n",
    "    # Creating PyTorch DataLoaders\n",
    "    train_data_loader = data.DataLoader(\n",
    "        dataset=train_dataset, batch_size=args.BATCH_SIZE, shuffle=True)\n",
    "    valid_data_loader = data.DataLoader(\n",
    "        dataset=valid_dataset, batch_size=args.BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    # device (cpu/gpu)\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # instatiate model and sending it to device\n",
    "    model = MnistModel(classes=classes).to(device)\n",
    "    # instantiate optimizer\n",
    "    optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
    "    # instantiate scheduler\n",
    "    scheduler = optim.lr_scheduler.CyclicLR(\n",
    "        optimizer, base_lr=args.lr, max_lr=0.1)\n",
    "\n",
    "    print('Training..')\n",
    "    best_accuracy = 0\n",
    "    # loop through epochs\n",
    "    for epoch in range(args.NUM_EPOCHS):\n",
    "        print(f'Epoch [{epoch+1}/{args.NUM_EPOCHS}]')\n",
    "        # train on train data\n",
    "        train_loop_fn(train_data_loader, model, optimizer, device, scheduler)\n",
    "        # evaluate on validation data\n",
    "        o, t = eval_loop_fn(valid_data_loader, model, device)\n",
    "        accuracy = (o == t).mean() * 100\n",
    "        print(f'Accuracy on Valid Data : {accuracy} %')\n",
    "        if accuracy > best_accuracy:\n",
    "            torch.save(model.state_dict(), args.model_path)\n",
    "            best_accuracy = accuracy\n",
    "\n",
    "    # Predict on test data\n",
    "    return test_loop_fn(df_test, model, device)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # variables for training model\n",
    "    @dataclass\n",
    "    class Args:\n",
    "        lr: float = 3e-5\n",
    "        RANDOM_STATE: int = 42\n",
    "        NUM_EPOCHS: int = 5\n",
    "        BATCH_SIZE: int = 100\n",
    "        target: str = 'label'\n",
    "        data_path: str = 'data/'\n",
    "        model_path: str = 'checkpoint/mnist.pt'\n",
    "\n",
    "        def __post_init__(self):\n",
    "            makedirs('checkpoint', exist_ok=True)\n",
    "\n",
    "    arg = Args()\n",
    "    random_seed(arg.RANDOM_STATE)\n",
    "    test_preds = run(args=arg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a069b0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
